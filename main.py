\nimport os\nimport json\nimport logging\nimport sentry_sdk\nimport requests\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime\nimport asyncio\nimport aiohttp\nimport httpx\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException, Depends, Request\nfrom pydantic import BaseModel, Field, HttpUrl, validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom sentence_transformers import SentenceTransformer\nfrom pinecone import Pinecone, ServerlessSpec\nfrom crewai import Agent, Task, Crew\nfrom dotenv import load_dotenv\nfrom functools import wraps\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nload_dotenv()\n\n\n\nVECTOR_IDS_FILE = "vector_ids.json"\nMAX_RETRIES = 3\nBATCH_SIZE = 100\n\n\n# Sentry Configuration\n\nclass SentryConfig:\n    """\n    Class to encapsulate Sentry SDK initialization and event handling.\n    """\n    @classmethod\n    def init_sentry(cls):\n        """\n        Initializes Sentry with environment-based configuration.\n        """\n        dsn = os.getenv("SENTRY_DSN")\n        if not dsn:\n            logger.warning("SENTRY_DSN not found in environment. Sentry will not be enabled.")\n            return\n\n        sentry_sdk.init(\n            dsn=dsn,\n            debug=os.getenv("SENTRY_DEBUG", "false").lower() == "true",\n            environment=os.getenv("SENTRY_ENVIRONMENT", "production"),\n            traces_sample_rate=float(os.getenv("SENTRY_SAMPLE_RATE", "1.0")),\n            profiles_sample_rate=float(os.getenv("SENTRY_ERROR_SAMPLE_RATE", "1.0")),\n            send_default_pii=True,\n            attach_stacktrace=True,\n            include_source_context=True,\n            include_local_variables=True,\n            max_breadcrumbs=50,\n            server_name=os.getenv("SERVER_NAME", "fastapi-server"),\n            before_send=cls.before_send,\n        )\n        logger.info("Sentry successfully initialized.")\n\n    @staticmethod\n    def before_send(event, hint):\n        """\n        Scrubs sensitive information before sending an event to Sentry.\n        Can be customized to exclude certain errors.\n        """\n        if "exc_info" in hint:\n            exc_type, exc_value, _ = hint["exc_info"]\n            if isinstance(exc_value, HTTPException) and exc_value.status_code in [400, 404]:  # Ignore common errors\n                return None  \n        return event\n\n\n\n\n\nclass ConfigModel(BaseModel):\n    gemini_api_key: str\n    github_repo_base: HttpUrl\n    pinecone_api_key: str\n    pinecone_environment: str = Field(default="us-east-1")\n    pinecone_index_name: str = Field(default="my-custom-index")\n    pinecone_endpoint: Optional[HttpUrl] = None\n    github_token: Optional[str] = None\n    max_workers: int = Field(default=4, ge=1, le=10)\n\nclass Config:\n    def __init__(self):\n        self.gemini_api_key = os.getenv("GEMINI_API_KEY")\n        self.github_repo_base = os.getenv("GITHUB_REPO_BASE")\n        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")\n        self.pinecone_environment = os.getenv("PINECONE_ENVIRONMENT", "us-east-1")\n        self.pinecone_index_name = os.getenv("PINECONE_INDEX_NAME", "my-custom-index")\n        self.pinecone_endpoint = os.getenv("PINECONE_ENDPOINT")\n        self.github_token = os.getenv("GITHUB_TOKEN")\n        self.max_workers = int(os.getenv("MAX_WORKERS", "4"))\n        self._validate_config()\n        \n        self.config_model = ConfigModel(\n            gemini_api_key=self.gemini_api_key,\n            github_repo_base=self.github_repo_base,\n            pinecone_api_key=self.pinecone_api_key,\n            pinecone_environment=self.pinecone_environment,\n            pinecone_index_name=self.pinecone_index_name,\n            pinecone_endpoint=self.pinecone_endpoint,\n            github_token=self.github_token,\n            max_workers=self.max_workers\n        )\n\n    def _validate_config(self):\n        required_vars = {\n            "GEMINI_API_KEY": self.gemini_api_key,\n            "GITHUB_REPO_BASE": self.github_repo_base,\n            "PINECONE_API_KEY": self.pinecone_api_key\n        }\n        missing_vars = [var for var, value in required_vars.items() if not value]\n        if missing_vars:\n            raise ValueError(f"Missing required environment variables: {\', \'.join(missing_vars)}")\n\nclass DocumentSection(BaseModel):\n    content: str\n    doc_name: str\n    section_index: int\n\nclass DocumentationManager:\n    def __init__(self, config: Config):\n        self.config = config\n        self.embedder = SentenceTransformer(\'paraphrase-mpnet-base-v2\')\n        self.last_processed_timestamp = datetime.now()\n        self._init_pinecone()\n        self._load_vector_ids()\n\n    def _init_pinecone(self) -> None:\n        for attempt in range(MAX_RETRIES):\n            try:\n                self.pc = Pinecone(\n                    api_key=self.config.pinecone_api_key,\n                    environment=self.config.pinecone_environment,\n                    endpoint=self.config.pinecone_endpoint\n                )\n                existing_indexes = self.pc.list_indexes().names()\n                logger.info(f"Existing indexes: {existing_indexes}")\n\n                if self.config.pinecone_index_name not in existing_indexes:\n                    logger.info(f"Creating new Pinecone index: {self.config.pinecone_index_name}")\n                    self.pc.create_index(\n                        name=self.config.pinecone_index_name,\n                        dimension=768,\n                        metric=\'cosine\',\n                        spec=ServerlessSpec(cloud=\'gcp\', region=\'us-east-1\')\n                    )\n                \n                self.index = self.pc.Index(self.config.pinecone_index_name)\n                break\n            except Exception as e:\n                if attempt == MAX_RETRIES - 1:\n                    logger.error(f"Failed to initialize Pinecone after {MAX_RETRIES} attempts: {str(e)}")\n                    raise HTTPException(status_code=500, detail=f"Pinecone initialization failed: {str(e)}")\n                logger.warning(f"Retrying Pinecone initialization. Attempt {attempt + 1}/{MAX_RETRIES}")\n                asyncio.sleep(1)\n\n    def _load_vector_ids(self) -> None:\n        try:\n            if os.path.exists(VECTOR_IDS_FILE):\n                with open(VECTOR_IDS_FILE, "r", encoding="utf-8") as f:\n                    self.pinecone_ids = json.load(f)\n                logger.info(f"Loaded {len(self.pinecone_ids)} vector IDs")\n            else:\n                self.pinecone_ids = []\n        except Exception as e:\n            logger.error(f"Error loading vector IDs: {str(e)}")\n            self.pinecone_ids = []\n\n    async def fetch_markdown_files(self, folder_path: str = "") -> Dict[str, str]:\n        async def fetch_file(session: aiohttp.ClientSession, url: str, filename: str) -> tuple[str, str]:\n            async with session.get(url) as response:\n                response.raise_for_status()\n                content = await response.text()\n                return filename, content\n\n        docs_content = {}\n        headers = {"Authorization": f"Bearer {self.config.github_token}"} if self.config.github_token else {}\n        \n        async with aiohttp.ClientSession(headers=headers) as session:\n            try:\n                current_repo_url = (f"{self.config.github_repo_base}/contents/{folder_path}" \n                                  if folder_path else f"{self.config.github_repo_base}/contents")\n                \n                async with session.get(current_repo_url) as response:\n                    response.raise_for_status()\n                    files = await response.json()\n\n                tasks = []\n                for file_info in files:\n                    if file_info[\'type\'] == \'dir\':\n                        new_folder_path = os.path.join(folder_path, file_info[\'name\'])\n                        nested_content = await self.fetch_markdown_files(new_folder_path)\n                        docs_content.update(nested_content)\n                    elif file_info[\'name\'].endswith(\'.md\'):\n                        tasks.append(\n                            fetch_file(session, file_info[\'download_url\'], file_info[\'name\'])\n                        )\n\n                results = await asyncio.gather(*tasks, return_exceptions=True)\n                for result in results:\n                    if isinstance(result, Exception):\n                        logger.error(f"Error fetching file: {str(result)}")\n                    else:\n                        filename, content = result\n                        docs_content[filename] = content\n\n                return docs_content\n\n            except aiohttp.ClientError as e:\n                logger.error(f"Error fetching markdown files: {str(e)}")\n                raise HTTPException(\n                    status_code=e.response.status_code if hasattr(e, \'response\') else 500,\n                    detail=str(e)\n                )\n\n    def _embed_section(self, section: DocumentSection) -> dict:\n        embedding = self.embedder.encode(section.content)\n        return {\n            \'id\': f"{section.doc_name}-{section.section_index}",\n            \'values\': embedding.tolist(),\n            \'metadata\': {\n                \'doc_name\': section.doc_name,\n                \'section_index\': section.section_index,\n                \'content\': section.content,\n                \'timestamp\': self.last_processed_timestamp.isoformat()\n            }\n        }\n\n    def _upsert_batch(self, vectors: List[dict]) -> None:\n        for attempt in range(MAX_RETRIES):\n            try:\n                self.index.upsert(vectors=vectors)\n                return\n            except Exception as e:\n                if attempt == MAX_RETRIES - 1:\n                    logger.error(f"Failed to upsert batch after {MAX_RETRIES} attempts: {str(e)}")\n                    raise\n                logger.warning(f"Retrying batch upsert. Attempt {attempt + 1}/{MAX_RETRIES}")\n                asyncio.sleep(1)\n\n    def process_documentation(self, docs_content: Dict[str, str]) -> None:\n        try:\n            self.last_processed_timestamp = datetime.now()\n            self.pinecone_ids = []\n            vectors_to_upsert = []\n\n            for doc_name, content in docs_content.items():\n                # Store complete document content\n                doc_section = DocumentSection(\n                    content=content,\n                    doc_name=doc_name,\n                    section_index=0\n                )\n\n                vector = self._embed_section(doc_section)\n                vectors_to_upsert.append(vector)\n                self.pinecone_ids.append(vector[\'id\'])\n\n                if len(vectors_to_upsert) >= BATCH_SIZE:\n                    self._upsert_batch(vectors_to_upsert)\n                    vectors_to_upsert = []\n\n            if vectors_to_upsert:\n                self._upsert_batch(vectors_to_upsert)\n\n            with open(VECTOR_IDS_FILE, "w", encoding="utf-8") as f:\n                json.dump(self.pinecone_ids, f)\n\n            logger.info(f"Successfully processed {len(self.pinecone_ids)} documents")\n\n        except Exception as e:\n            logger.error(f"Error in process_documentation: {str(e)}")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def retrieve_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        try:\n            query_embedding = self.embedder.encode(query).tolist()\n\n            response = self.index.query(\n                vector=query_embedding,\n                top_k=top_k,\n                include_metadata=True\n            )\n\n            matching_docs = []\n            for match in response[\'matches\']:\n                matching_docs.append({\n                    "doc_name": match[\'metadata\'][\'doc_name\'],\n                    "similarity_score": match[\'score\'],\n                    "processed_at": match[\'metadata\'].get(\'timestamp\', \'\'),\n                    "content": match[\'metadata\'].get(\'content\', "")\n                })\n\n            return matching_docs\n\n        except Exception as e:\n            logger.error(f"Error retrieving documents: {str(e)}")\n            raise HTTPException(status_code=500, detail=f"Document retrieval failed: {str(e)}")\n\n# Initialize FastAPI app\napp = FastAPI(\n    title="Documentation Assistant API",\n    description="AI-powered documentation assistant with complete content preservation",\n    version="2.0.0"\n)\n\nclass QueryInput(BaseModel):\n    """Validation model for query inputs"""\n    query: str = Field(..., min_length=3, max_length=1000)\n    user_context: Optional[str] = Field(default="", max_length=500)\n\n    class Config:\n        json_schema_extra = {\n            "example": {\n                "query": "How do I setup the application?",\n                "user_context": "I\'m a new developer trying to understand the system."\n            }\n        }\n\nclass CrewManager:\n    """Manages CrewAI agents and tasks"""\n    def __init__(self, config: Config):\n        self.config = config\n        self.setup_agents()\n        self.setup_tasks()\n\n    def setup_agents(self):\n        """Initialize CrewAI agents"""\n        self.agents = {\n            \'retriever\': Agent(\n                role="Documentation Retriever",\n                goal="Identify and fetch relevant documentation sections based on user queries.",\n                backstory="A specialized AI designed to efficiently search documentation using embeddings.",\n                tools=[],\n                verbose=True,\n                memory=True,\n                llm="gemini/gemini-2.0-flash"\n            ),\n            \'analyzer\': Agent(\n                role="Content Analyzer",\n                goal="Process retrieved documentation to extract structured insights.",\n                backstory="An AI that specializes in organizing and structuring knowledge for easy consumption.",\n                tools=[],\n                verbose=True,\n                memory=True,\n                llm="gemini/gemini-2.0-flash"\n            ),\n            \'assistant\': Agent(\n                role="Documentation Guide",\n                goal="Provide clear and well-structured answers based on relevant documentation.",\n                backstory="A helpful AI that presents technical documentation in an easy-to-understand way.",\n                verbose=True,\n                memory=True,\n                llm="gemini/gemini-2.0-flash"\n            )\n        }\n\n    def setup_tasks(self):\n        """Initialize tasks"""\n        self.tasks = [\n            Task(\n                description="Retrieve the most relevant documentation sections for a given query using vector search.",\n                expected_output="A list of relevant documentation sections.",\n                agent=self.agents[\'retriever\']\n            ),\n            Task(\n                description="Analyze and summarize the retrieved documentation sections, extracting key points.",\n                expected_output="A structured summary of key insights from the retrieved sections.",\n                agent=self.agents[\'analyzer\']\n            ),\n            Task(\n                description="Generate a well-structured response for the user, incorporating retrieved insights.",\n                expected_output="A detailed response with references to specific documentation sections.",\n                agent=self.agents[\'assistant\']\n            )\n        ]\n\n    def create_query_tasks(self, query: str, user_context: str, matching_docs: Dict[str, Any]) -> List[Task]:\n        """Create specialized tasks for a specific query"""\n        return [\n            Task(\n                description=f"""\n                    Identify and fetch the most relevant documentation sections based on the user\'s query.\n                    User Query: {query}\n                    User Context: {user_context}\n                """,\n                expected_output="A list of relevant documentation sections with metadata.",\n                agent=self.agents[\'retriever\']\n            ),\n            Task(\n                description=f"""\n                    Analyze the retrieved documentation sections and summarize the key insights.\n                    Query: {query}\n                    Relevant Documentation: {json.dumps(matching_docs)}\n                """,\n                expected_output="A well-structured summary of key findings from the retrieved sections.",\n                agent=self.agents[\'analyzer\']\n            ),\n            Task(\n                description=f"""\n                    Formulate a detailed and structured response to the user\'s query, referencing relevant documentation.\n                    Ensure the answer is easy to understand and includes direct citations.\n                    Query: {query}\n                    Summary from Analysis: [Include key points]\n                """,\n                expected_output="A clear, structured response to the user\'s query with references.",\n                agent=self.agents[\'assistant\']\n            )\n        ]\n        # Dependency injection functions\ndef get_config() -> Config:\n    return _config_instance\n\ndef get_doc_manager(config: Config = Depends(get_config)) -> DocumentationManager:\n    return _doc_manager_instance\n\ndef get_crew_manager(config: Config = Depends(get_config)) -> CrewManager:\n    return _crew_manager_instance\n\n# Add CrewManager dependency\nasync def get_crew_manager(config: Config = Depends(get_config)) -> CrewManager:\n    return _crew_manager_instance\n\n# Initialize singleton instances\n_config_instance = Config()\n_doc_manager_instance = DocumentationManager(_config_instance)\n_crew_manager_instance = CrewManager(_config_instance)\n\n# Initialize FastAPI app\napp = FastAPI(\n    title="Documentation Assistant API",\n    description="AI-powered documentation assistant with complete content preservation",\n    version="2.0.0"\n)\n# Initialize Sentry\nSentryConfig.init_sentry()\n\n\n\n# Custom error-handling decorator for API routes\ndef sentry_error_handler(func):\n    """\n    Decorator to wrap FastAPI route handlers and send errors to Sentry.\n    """\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        try:\n            return await func(*args, **kwargs)\n        except Exception as e:\n            sentry_sdk.capture_exception(e)\n            logger.error(f"Error in {func.__name__}: {str(e)}")\n            raise HTTPException(status_code=500, detail="An unexpected error occurred.")\n    return wrapper\n\n# Add CrewAI endpoint\n\n@app.get("/", tags=["General"])\nasync def root():\n    return {\n        "message": "Welcome to the Documentation Assistant API",\n        "version": "2.0.0",\n        "status": "operational",\n        "timestamp": datetime.now().isoformat()\n    }\n\n@app.get("/fetch_docs", tags=["Documentation"])\nasync def fetch_docs(\n    doc_manager: DocumentationManager = Depends(get_doc_manager)\n):\n    try:\n        logger.info("Starting documentation fetch process...")\n        docs_content = await doc_manager.fetch_markdown_files()\n        \n        if not docs_content:\n            raise HTTPException(status_code=404, detail="No documentation found")\n        \n        doc_manager.process_documentation(docs_content)\n        \n        return {\n            "status": "success",\n            "message": "Documentation fetched and indexed",\n            "stats": {\n                "files_processed": len(docs_content),\n                "files": list(docs_content.keys()),\n                "processed_at": datetime.now().isoformat()\n            }\n        }\n    except Exception as e:\n        logger.error(f"Error in documentation fetch: {str(e)}")\n        raise HTTPException(status_code=500, detail=f"Documentation fetch failed: {str(e)}")\n\n@app.post("/ask_doc_assistant", tags=["Query"])\nasync def ask_doc_assistant(\n    query_input: QueryInput,\n    doc_manager: DocumentationManager = Depends(get_doc_manager),\n    crew_manager: CrewManager = Depends(get_crew_manager)\n):\n    """\n    Process user queries using RAG with CrewAI-powered analysis.\n    """\n    try:\n        logger.info(f"Processing query: {query_input.query}")\n        query_start_time = datetime.now()\n\n        # Retrieve relevant documentation\n        matching_docs = await doc_manager.retrieve_documents(\n            query=query_input.query,\n            top_k=15\n        )\n\n        if not matching_docs:\n            return {\n                "status": "no_results",\n                "response": "No relevant documentation found for your query.",\n                "query_context": {\n                    "original_query": query_input.query,\n                    "user_context": query_input.user_context\n                },\n                "matching_docs": []\n            }\n\n        # Create and execute CrewAI tasks\n        query_tasks = crew_manager.create_query_tasks(\n            query_input.query,\n            query_input.user_context,\n            matching_docs\n        )\n\n        crew = Crew(\n            agents=[\n                crew_manager.agents[\'retriever\'],\n                crew_manager.agents[\'analyzer\'],\n                crew_manager.agents[\'assistant\']\n            ],\n            tasks=query_tasks,\n            verbose=True\n        )\n        \n        result = crew.kickoff()\n\n        structured_response = {\n            "status": "success",\n            "response": result,\n            "query_context": {\n                "original_query": query_input.query,\n                "user_context": query_input.user_context,\n                "processing_time": (datetime.now() - query_start_time).total_seconds()\n            },\n            "matching_docs": [\n                {\n                    "doc_name": doc["doc_name"],\n                    "similarity_score": doc["similarity_score"],\n                    "processed_at": doc["processed_at"],\n                    "content": doc["content"]  # Return full content\n                }\n                for doc in matching_docs\n            ]\n        }\n\n        logger.info("Query processed successfully")\n        return structured_response\n\n    except Exception as e:\n        logger.error(f"Error processing query: {str(e)}")\n        raise HTTPException(\n            status_code=500,\n            detail={\n                "error": "Query processing failed",\n                "message": str(e),\n                "query": query_input.query\n            }\n        )\n\n@app.delete("/clear_index", tags=["Maintenance"])\nasync def clear_index(\n    doc_manager: DocumentationManager = Depends(get_doc_manager)\n):\n    try:\n        logger.info("Clearing Pinecone index...")\n        doc_manager.index.delete(delete_all=True)\n        doc_manager.pinecone_ids = []\n        with open(VECTOR_IDS_FILE, "w", encoding="utf-8") as f:\n            json.dump([], f)\n        return {"status": "success", "message": "Index cleared successfully"}\n    except Exception as e:\n        logger.error(f"Error clearing index: {str(e)}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get("/sentry-debug", tags=["Sentry"])\nasync def trigger_error():\n    # Fix: Check if the divisor is zero before dividing\n    divisor = 0\n    if divisor == 0:\n        return {"error": "Cannot divide by zero"}\n    else:\n        division_by_zero = 1 / divisor\n\n\nSENTRY_AUTH_TOKEN = os.getenv("SENTRY_AUTH_TOKEN")\nORG_SLUG = os.getenv("ORG_SLUG")\nPROJECT_SLUG = os.getenv("PROJECT_SLUG")\nGITHUB_WEBHOOK_URL = os.getenv("GITHUB_WEBHOOK_URL")\nGITHUB_PAT = os.getenv("GITHUB_PAT")\n\n# Processed issues (to avoid duplicate branches)\nprocessed_issues = set()\n\nclass SentryAPI:\n    def __init__(self, auth_token: str, organization_slug: str, project_slug: str):\n        self.auth_token = auth_token\n        self.base_url = "https://sentry.io/api/0"\n        self.org_slug = organization_slug\n        self.project_slug = project_slug\n        self.headers = {\n            \'Authorization\': f\'Bearer {auth_token}\',\n            \'Content-Type\': \'application/json\'\n        }\n\n    def _get(self, endpoint: str, params: Optional[Dict] = None) -> Dict:\n        """Helper method for making GET requests to Sentry API."""\n        url = f"{self.base_url}{endpoint}"\n        response = requests.get(url, headers=self.headers, params=params)\n        if response.status_code != 200:\n            raise HTTPException(status_code=response.status_code, detail="Failed to fetch data from Sentry")\n        return response.json()\n\n    def get_latest_issues(self, limit: int = 100) -> List[Dict]:\n        """Fetch the latest unresolved issues from Sentry."""\n        endpoint = f"/projects/{self.org_slug}/{self.project_slug}/issues/"\n        params = {\'limit\': limit, \'query\': \'is:unresolved\'}\n        return self._get(endpoint, params)\n\n    def get_error_location(self, event_id: str) -> Dict:\n        """Get the specific error location including file, line number, and code context."""\n        endpoint = f"/projects/{self.org_slug}/{self.project_slug}/events/{event_id}/"\n        event_data = self._get(endpoint)\n\n        error_location = {\n            \'file\': None,\n            \'line_number\': None,\n            \'context_lines\': [],\n            \'error_line\': None,\n            \'function\': None,\n            \'pre_context\': [],\n            \'post_context\': []\n        }\n\n        if \'entries\' in event_data:\n            for entry in event_data.get(\'entries\', []):\n                if entry.get(\'type\') == \'exception\':\n                    frames = entry.get(\'data\', {}).get(\'values\', [])[0].get(\'stacktrace\', {}).get(\'frames\', [])\n                    if frames:\n                        last_frame = frames[-1]\n                        error_location.update({\n                            \'file\': last_frame.get(\'filename\'),\n                            \'line_number\': last_frame.get(\'lineno\'),\n                            \'context_lines\': last_frame.get(\'context_line\'),\n                            \'error_line\': last_frame.get(\'context_line\'),\n                            \'function\': last_frame.get(\'function\'),\n                            \'pre_context\': last_frame.get(\'pre_context\', []),\n                            \'post_context\': last_frame.get(\'post_context\', [])\n                        })\n        return error_location\n\n    def get_full_error_details(self, issue_id: str) -> Dict:\n        """Get comprehensive error details including stack trace and error location."""\n        endpoint = f"/issues/{issue_id}/events/latest/"\n        event_data = self._get(endpoint)\n\n        error_details = {\n            \'error_type\': None,\n            \'error_message\': None,\n            \'error_location\': None,\n            \'stack_trace\': [],\n            \'timestamp\': None\n        }\n\n        if \'entries\' in event_data:\n            for entry in event_data.get(\'entries\', []):\n                if entry.get(\'type\') == \'exception\':\n                    exception = entry.get(\'data\', {}).get(\'values\', [])[0]\n                    error_details.update({\n                        \'error_type\': exception.get(\'type\'),\n                        \'error_message\': exception.get(\'value\'),\n                        \'timestamp\': event_data.get(\'dateCreated\')\n                    })\n                    frames = exception.get(\'stacktrace\', {}).get(\'frames\', [])\n                    error_details[\'stack_trace\'] = [\n                        {\n                            \'filename\': frame.get(\'filename\'),\n                            \'function\': frame.get(\'function\'),\n                            \'line_number\': frame.get(\'lineno\'),\n                            \'context_line\': frame.get(\'context_line\')\n                        }\n                        for frame in frames\n                    ]\n        error_details[\'error_location\'] = self.get_error_location(event_data.get(\'eventID\'))\n        return error_details\n\n\nclass ErrorDetailResponse(BaseModel):\n    error_type: Optional[str]\n    error_message: Optional[str]\n    error_location: Optional[Dict]\n    stack_trace: List[Dict]\n    timestamp: Optional[str]\n\n\n# Initialize Sentry API\nsentry_api = SentryAPI(auth_token=SENTRY_AUTH_TOKEN, organization_slug=ORG_SLUG, project_slug=PROJECT_SLUG)\n\n\nasync def check_sentry_issues():\n    """Periodically checks for new Sentry issues and creates GitHub branches."""\n    while True:\n        try:\n            issues = sentry_api.get_latest_issues(limit=100)\n            async with httpx.AsyncClient() as client:\n                for issue in issues:\n                    issue_id = issue["id"]\n\n                    if issue_id in processed_issues:\n                        continue  # Skip if the issue was already processed\n\n                    title = issue["title"]\n                    permalink = issue["permalink"]\n                    error_details = sentry_api.get_full_error_details(issue_id)\n\n                    # Prepare payload for GitHub repository dispatch event\n                    webhook_payload = {\n                        "event_type": "create-branch",\n                        "client_payload": {\n                            "branch_name": issue_id,  # Branch name will be the Sentry issue ID\n                            "issue_title": title,\n                            "permalink": permalink\n                        }\n                    }\n\n                    # Send the POST request to trigger the GitHub Action\n                    headers = {\n                        "Authorization": f"token {GITHUB_PAT}",\n                        "Accept": "application/vnd.github.v3+json"\n                    }\n                    webhook_response = await client.post(GITHUB_WEBHOOK_URL, json=webhook_payload, headers=headers)\n\n                    if webhook_response.status_code == 204:\n                        print(f"✅ Branch creation triggered for issue {issue_id}")\n                        processed_issues.add(issue_id)  # Mark issue as processed\n                    else:\n                        print(f"❌ Webhook call failed for issue {issue_id}: {webhook_response.status_code} {webhook_response.text}")\n\n        except Exception as e:\n            print(f"⚠️ Error fetching Sentry issues: {str(e)}")\n\n        await asyncio.sleep(60)  # Wait 60 seconds before checking again\n\n\n@app.on_event("startup")\nasync def startup_event():\n    """Start checking for Sentry issues when FastAPI starts."""\n    asyncio.create_task(check_sentry_issues())\n\n\n@app.get("/status")\nasync def get_status():\n    """Check how many issues have been processed."""\n    return {"processed_issues": list(processed_issues)}\n\n\n@app.get("/get-sentry-issues")\nasync def get_sentry_issues(limit: int = 100):\n    issues = sentry_api.get_latest_issues(limit=limit)\n    formatted_issues = []\n    \n    # Use an asynchronous HTTP client to send webhook calls\n    async with httpx.AsyncClient() as client:\n        for issue in issues:\n            issue_id = issue["id"]\n            title = issue["title"]\n            permalink = issue["permalink"]\n            error_details = sentry_api.get_full_error_details(issue_id)\n\n            formatted_issue = {\n                "id": issue_id,\n                "title": title,\n                "permalink": permalink,\n                "error_type": error_details[\'error_type\'],\n                "error_message": error_details[\'error_message\'],\n                "error_location": error_details[\'error_location\'],\n                "timestamp": error_details[\'timestamp\']\n            }\n            formatted_issues.append(formatted_issue)\n\n            # Prepare payload for GitHub repository dispatch event\n            webhook_payload = {\n                "event_type": "create-branch",\n                "client_payload": {\n                    "branch_name": issue_id,  # The branch name will be the Sentry issue ID\n                    "issue_title": title,\n                    "permalink": permalink\n                }\n            }\n            # Send the POST request to trigger the GitHub Action\n            headers = {\n                "Authorization": f"token {GITHUB_PAT}",\n                "Accept": "application/vnd.github.v3+json"\n            }\n            webhook_response = await client.post(GITHUB_WEBHOOK_URL, json=webhook_payload, headers=headers)\n            if webhook_response.status_code != 204:\n                # GitHub API returns 204 on success for repository_dispatch events\n                print(f"Webhook call failed for issue {issue_id}: {webhook_response.status_code} {webhook_response.text}")\n\n    return {"status": "success", "issues": formatted_issues}\n\n\n@app.get("/get-sentry-error-details/{issue_id}")\nasync def get_error_details(issue_id: str):\n    """Fetches detailed error information for a specific Sentry issue in the same format as `/get-sentry-issues`."""\n    \n    # Fetch error details from Sentry API\n    error_details = sentry_api.get_full_error_details(issue_id)\n    \n    # If issue does not exist, return 404\n    if not error_details.get("error_type"):\n        return'), ('pydantic', None), ('json_dict', None), ('tasks_output', [TaskOutput(description='Analyze and fix the issue in the following code:\n\nimport os\nimport json\nimport logging\nimport sentry_sdk\nimport requests\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime\nimport asyncio\nimport aiohttp\nimport httpx\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException, Depends, Request\nfrom pydantic import BaseModel, Field, HttpUrl, validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom sentence_transformers import SentenceTransformer\nfrom pinecone import Pinecone, ServerlessSpec\nfrom crewai import Agent, Task, Crew\nfrom dotenv import load_dotenv\nfrom functools import wraps\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nload_dotenv()\n\n\n\nVECTOR_IDS_FILE = "vector_ids.json"\nMAX_RETRIES = 3\nBATCH_SIZE = 100\n\n\n# Sentry Configuration\n\nclass SentryConfig:\n    """\n    Class to encapsulate Sentry SDK initialization and event handling.\n    """\n    @classmethod\n    def init_sentry(cls):\n        """\n        Initializes Sentry with environment-based configuration.\n        """\n        dsn = os.getenv("SENTRY_DSN")\n        if not dsn:\n            logger.warning("SENTRY_DSN not found in environment. Sentry will not be enabled.")\n            return\n\n        sentry_sdk.init(\n            dsn=dsn,\n            debug=os.getenv("SENTRY_DEBUG", "false").lower() == "true",\n            environment=os.getenv("SENTRY_ENVIRONMENT", "production"),\n            traces_sample_rate=float(os.getenv("SENTRY_SAMPLE_RATE", "1.0")),\n            profiles_sample_rate=float(os.getenv("SENTRY_ERROR_SAMPLE_RATE", "1.0")),\n            send_default_pii=True,\n            attach_stacktrace=True,\n            include_source_context=True,\n            include_local_variables=True,\n            max_breadcrumbs=50,\n            server_name=os.getenv("SERVER_NAME", "fastapi-server"),\n            before_send=cls.before_send,\n        )\n        logger.info("Sentry successfully initialized.")\n\n    @staticmethod\n    def before_send(event, hint):\n        """\n        Scrubs sensitive information before sending an event to Sentry.\n        Can be customized to exclude certain errors.\n        """\n        if "exc_info" in hint:\n            exc_type, exc_value, _ = hint["exc_info"]\n            if isinstance(exc_value, HTTPException) and exc_value.status_code in [400, 404]:  # Ignore common errors\n                return None  \n        return event\n\n\n\n\n\nclass ConfigModel(BaseModel):\n    gemini_api_key: str\n    github_repo_base: HttpUrl\n    pinecone_api_key: str\n    pinecone_environment: str = Field(default="us-east-1")\n    pinecone_index_name: str = Field(default="my-custom-index")\n    pinecone_endpoint: Optional[HttpUrl] = None\n    github_token: Optional[str] = None\n    max_workers: int = Field(default=4, ge=1, le=10)\n\nclass Config:\n    def __init__(self):\n        self.gemini_api_key = os.getenv("GEMINI_API_KEY")\n        self.github_repo_base = os.getenv("GITHUB_REPO_BASE")\n        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")\n        self.pinecone_environment = os.getenv("PINECONE_ENVIRONMENT", "us-east-1")\n        self.pinecone_index_name = os.getenv("PINECONE_INDEX_NAME", "my-custom-index")\n        self.pinecone_endpoint = os.getenv("PINECONE_ENDPOINT")\n        self.github_token = os.getenv("GITHUB_TOKEN")\n        self.max_workers = int(os.getenv("MAX_WORKERS", "4"))\n        self._validate_config()\n        \n        self.config_model = ConfigModel(\n            gemini_api_key=self.gemini_api_key,\n            github_repo_base=self.github_repo_base,\n            pinecone_api_key=self.pinecone_api_key,\n            pinecone_environment=self.pinecone_environment,\n            pinecone_index_name=self.pinecone_index_name,\n            pinecone_endpoint=self.pinecone_endpoint,\n            github_token=self.github_token,\n            max_workers=self.max_workers\n        )\n\n    def _validate_config(self):\n        required_vars = {\n            "GEMINI_API_KEY": self.gemini_api_key,\n            "GITHUB_REPO_BASE": self.github_repo_base,\n            "PINECONE_API_KEY": self.pinecone_api_key\n        }\n        missing_vars = [var for var, value in required_vars.items() if not value]\n        if missing_vars:\n            raise ValueError(f"Missing required environment variables: {\', \'.join(missing_vars)}")\n\nclass DocumentSection(BaseModel):\n    content: str\n    doc_name: str\n    section_index: int\n\nclass DocumentationManager:\n    def __init__(self, config: Config):\n        self.config = config\n        self.embedder = SentenceTransformer(\'paraphrase-mpnet-base-v2\')\n        self.last_processed_timestamp = datetime.now()\n        self._init_pinecone()\n        self._load_vector_ids()\n\n    def _init_pinecone(self) -> None:\n        for attempt in range(MAX_RETRIES):\n            try:\n                self.pc = Pinecone(\n                    api_key=self.config.pinecone_api_key,\n                    environment=self.config.pinecone_environment,\n                    endpoint=self.config.pinecone_endpoint\n                )\n                existing_indexes = self.pc.list_indexes().names()\n                logger.info(f"Existing indexes: {existing_indexes}")\n\n                if self.config.pinecone_index_name not in existing_indexes:\n                    logger.info(f"Creating new Pinecone index: {self.config.pinecone_index_name}")\n                    self.pc.create_index(\n                        name=self.config.pinecone_index_name,\n                        dimension=768,\n                        metric=\'cosine\',\n                        spec=ServerlessSpec(cloud=\'gcp\', region=\'us-east-1\')\n                    )\n                \n                self.index = self.pc.Index(self.config.pinecone_index_name)\n                break\n            except Exception as e:\n                if attempt == MAX_RETRIES - 1:\n                    logger.error(f"Failed to initialize Pinecone after {MAX_RETRIES} attempts: {str(e)}")\n                    raise HTTPException(status_code=500, detail=f"Pinecone initialization failed: {str(e)}")\n                logger.warning(f"Retrying Pinecone initialization. Attempt {attempt + 1}/{MAX_RETRIES}")\n                asyncio.sleep(1)\n\n    def _load_vector_ids(self) -> None:\n        try:\n            if os.path.exists(VECTOR_IDS_FILE):\n                with open(VECTOR_IDS_FILE, "r", encoding="utf-8") as f:\n                    self.pinecone_ids = json.load(f)\n                logger.info(f"Loaded {len(self.pinecone_ids)} vector IDs")\n            else:\n                self.pinecone_ids = []\n        except Exception as e:\n            logger.error(f"Error loading vector IDs: {str(e)}")\n            self.pinecone_ids = []\n\n    async def fetch_markdown_files(self, folder_path: str = "") -> Dict[str, str]:\n        async def fetch_file(session: aiohttp.ClientSession, url: str, filename: str) -> tuple[str, str]:\n            async with session.get(url) as response:\n                response.raise_for_status()\n                content = await response.text()\n                return filename, content\n\n        docs_content = {}\n        headers = {"Authorization": f"Bearer {self.config.github_token}"} if self.config.github_token else {}\n        \n        async with aiohttp.ClientSession(headers=headers) as session:\n            try:\n                current_repo_url = (f"{self.config.github_repo_base}/contents/{folder_path}" \n                                  if folder_path else f"{self.config.github_repo_base}/contents")\n                \n                async with session.get(current_repo_url) as response:\n                    response.raise_for_status()\n                    files = await response.json()\n\n                tasks = []\n                for file_info in files:\n                    if file_info[\'type\'] == \'dir\':\n                        new_folder_path = os.path.join(folder_path, file_info[\'name\'])\n                        nested_content = await self.fetch_markdown_files(new_folder_path)\n                        docs_content.update(nested_content)\n                    elif file_info[\'name\'].endswith(\'.md\'):\n                        tasks.append(\n                            fetch_file(session, file_info[\'download_url\'], file_info[\'name\'])\n                        )\n\n                results = await asyncio.gather(*tasks, return_exceptions=True)\n                for result in results:\n                    if isinstance(result, Exception):\n                        logger.error(f"Error fetching file: {str(result)}")\n                    else:\n                        filename, content = result\n                        docs_content[filename] = content\n\n                return docs_content\n\n            except aiohttp.ClientError as e:\n                logger.error(f"Error fetching markdown files: {str(e)}")\n                raise HTTPException(\n                    status_code=e.response.status_code if hasattr(e, \'response\') else 500,\n                    detail=str(e)\n                )\n\n    def _embed_section(self, section: DocumentSection) -> dict:\n        embedding = self.embedder.encode(section.content)\n        return {\n            \'id\': f"{section.doc_name}-{section.section_index}",\n            \'values\': embedding.tolist(),\n            \'metadata\': {\n                \'doc_name\': section.doc_name,\n                \'section_index\': section.section_index,\n                \'content\': section.content,\n                \'timestamp\': self.last_processed_timestamp.isoformat()\n            }\n        }\n\n    def _upsert_batch(self, vectors: List[dict]) -> None:\n        for attempt in range(MAX_RETRIES):\n            try:\n                self.index.upsert(vectors=vectors)\n                return\n            except Exception as e:\n                if attempt == MAX_RETRIES - 1:\n                    logger.error(f"Failed to upsert batch after {MAX_RETRIES} attempts: {str(e)}")\n                    raise\n                logger.warning(f"Retrying batch upsert. Attempt {attempt + 1}/{MAX_RETRIES}")\n                asyncio.sleep(1)\n\n    def process_documentation(self, docs_content: Dict[str, str]) -> None:\n        try:\n            self.last_processed_timestamp = datetime.now()\n            self.pinecone_ids = []\n            vectors_to_upsert = []\n\n            for doc_name, content in docs_content.items():\n                # Store complete document content\n                doc_section = DocumentSection(\n                    content=content,\n                    doc_name=doc_name,\n                    section_index=0\n                )\n\n                vector = self._embed_section(doc_section)\n                vectors_to_upsert.append(vector)\n                self.pinecone_ids.append(vector[\'id\'])\n\n                if len(vectors_to_upsert) >= BATCH_SIZE:\n                    self._upsert_batch(vectors_to_upsert)\n                    vectors_to_upsert = []\n\n            if vectors_to_upsert:\n                self._upsert_batch(vectors_to_upsert)\n\n            with open(VECTOR_IDS_FILE, "w", encoding="utf-8") as f:\n                json.dump(self.pinecone_ids, f)\n\n            logger.info(f"Successfully processed {len(self.pinecone_ids)} documents")\n\n        except Exception as e:\n            logger.error(f"Error in process_documentation: {str(e)}")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def retrieve_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n        try:\n            query_embedding = self.embedder.encode(query).tolist()\n\n            response = self.index.query(\n                vector=query_embedding,\n                top_k=top_k,\n                include_metadata=True\n            )\n\n            matching_docs = []\n            for match in response[\'matches\']:\n                matching_docs.append({\n                    "doc_name": match[\'metadata\'][\'doc_name\'],\n                    "similarity_score": match[\'score\'],\n                    "processed_at": match[\'metadata\'].get(\'timestamp\', \'\'),\n                    "content": match[\'metadata\'].get(\'content\', "")\n                })\n\n            return matching_docs\n\n        except Exception as e:\n            logger.error(f"Error retrieving documents: {str(e)}")\n            raise HTTPException(status_code=500, detail=f"Document retrieval failed: {str(e)}")\n\n# Initialize FastAPI app\napp = FastAPI(\n    title="Documentation Assistant API",\n    description="AI-powered documentation assistant with complete content preservation",\n    version="2.0.0"\n)\n\nclass QueryInput(BaseModel):\n    """Validation model for query inputs"""\n    query: str = Field(..., min_length=3, max_length=1000)\n    user_context: Optional[str] = Field(default="", max_length=500)\n\n    class Config:\n        json_schema_extra = {\n            "example": {\n                "query": "How do I setup the application?",\n                "user_context": "I\'m a new developer trying to understand the system."\n            }\n        }\n\nclass CrewManager:\n    """Manages CrewAI agents and tasks"""\n    def __init__(self, config: Config):\n        self.config = config\n        self.setup_agents()\n        self.setup_tasks()\n\n    def setup_agents(self):\n        """Initialize CrewAI agents"""\n        self.agents = {\n            \'retriever\': Agent(\n                role="Documentation Retriever",\n                goal="Identify and fetch relevant documentation sections based on user queries.",\n                backstory="A specialized AI designed to efficiently search documentation using embeddings.",\n                tools=[],\n                verbose=True,\n                memory=True,\n                llm="gemini/gemini-2.0-flash"\n            ),\n            \'analyzer\': Agent(\n                role="Content Analyzer",\n                goal="Process retrieved documentation to extract structured insights.",\n                backstory="An AI that specializes in organizing and structuring knowledge for easy consumption.",\n                tools=[],\n                verbose=True,\n                memory=True,\n                llm="gemini/gemini-2.0-flash"\n            ),\n            \'assistant\': Agent(\n                role="Documentation Guide",\n                goal="Provide clear and well-structured answers based on relevant documentation.",\n                backstory="A helpful AI that presents technical documentation in an easy-to-understand way.",\n                verbose=True,\n                memory=True,\n                llm="gemini/gemini-2.0-flash"\n            )\n        }\n\n    def setup_tasks(self):\n        """Initialize tasks"""\n        self.tasks = [\n            Task(\n                description="Retrieve the most relevant documentation sections for a given query using vector search.",\n                expected_output="A list of relevant documentation sections.",\n                agent=self.agents[\'retriever\']\n            ),\n            Task(\n                description="Analyze and summarize the retrieved documentation sections, extracting key points.",\n                expected_output="A structured summary of key insights from the retrieved sections.",\n                agent=self.agents[\'analyzer\']\n            ),\n            Task(\n                description="Generate a well-structured response for the user, incorporating retrieved insights.",\n                expected_output="A detailed response with references to specific documentation sections.",\n                agent=self.agents[\'assistant\']\n            )\n        ]\n\n    def create_query_tasks(self, query: str, user_context: str, matching_docs: Dict[str, Any]) -> List[Task]:\n        """Create specialized tasks for a specific query"""\n        return [\n            Task(\n                description=f"""\n                    Identify and fetch the most relevant documentation sections based on the user\'s query.\n                    User Query: {query}\n                    User Context: {user_context}\n                """,\n                expected_output="A list of relevant documentation sections with metadata.",\n                agent=self.agents[\'retriever\']\n            ),\n            Task(\n                description=f"""\n                    Analyze the retrieved documentation sections and summarize the key insights.\n                    Query: {query}\n                    Relevant Documentation: {json.dumps(matching_docs)}\n                """,\n                expected_output="A well-structured summary of key findings from the retrieved sections.",\n                agent=self.agents[\'analyzer\']\n            ),\n            Task(\n                description=f"""\n                    Formulate a detailed and structured response to the user\'s query, referencing relevant documentation.\n                    Ensure the answer is easy to understand and includes direct citations.\n                    Query: {query}\n                    Summary from Analysis: [Include key points]\n                """,\n                expected_output="A clear, structured response to the user\'s query with references.",\n                agent=self.agents[\'assistant\']\n            )\n        ]\n        # Dependency injection functions\ndef get_config() -> Config:\n    return _config_instance\n\ndef get_doc_manager(config: Config = Depends(get_config)) -> DocumentationManager:\n    return _doc_manager_instance\n\ndef get_crew_manager(config: Config = Depends(get_config)) -> CrewManager:\n    return _crew_manager_instance\n\n# Add CrewManager dependency\nasync def get_crew_manager(config: Config = Depends(get_config)) -> CrewManager:\n    return _crew_manager_instance\n\n# Initialize singleton instances\n_config_instance = Config()\n_doc_manager_instance = DocumentationManager(_config_instance)\n_crew_manager_instance = CrewManager(_config_instance)\n\n# Initialize FastAPI app\napp = FastAPI(\n    title="Documentation Assistant API",\n    description="AI-powered documentation assistant with complete content preservation",\n    version="2.0.0"\n)\n# Initialize Sentry\nSentryConfig.init_sentry()\n\n\n\n# Custom error-handling decorator for API routes\ndef sentry_error_handler(func):\n    """\n    Decorator to wrap FastAPI route handlers and send errors to Sentry.\n    """\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        try:\n            return await func(*args, **kwargs)\n        except Exception as e:\n            sentry_sdk.capture_exception(e)\n            logger.error(f"Error in {func.__name__}: {str(e)}")\n            raise HTTPException(status_code=500, detail="An unexpected error occurred.")\n    return wrapper\n\n# Add CrewAI endpoint\n\n@app.get("/", tags=["General"])\nasync def root():\n    return {\n        "message": "Welcome to the Documentation Assistant API",\n        "version": "2.0.0",\n        "status": "operational",\n        "timestamp": datetime.now().isoformat()\n    }\n\n@app.get("/fetch_docs", tags=["Documentation"])\nasync def fetch_docs(\n    doc_manager: DocumentationManager = Depends(get_doc_manager)\n):\n    try:\n        logger.info("Starting documentation fetch process...")\n        docs_content = await doc_manager.fetch_markdown_files()\n        \n        if not docs_content:\n            raise HTTPException(status_code=404, detail="No documentation found")\n        \n        doc_manager.process_documentation(docs_content)\n        \n        return {\n            "status": "success",\n            "message": "Documentation fetched and indexed",\n            "stats": {\n                "files_processed": len(docs_content),\n                "files": list(docs_content.keys()),\n                "processed_at": datetime.now().isoformat()\n            }\n        }\n    except Exception as e:\n        logger.error(f"Error in documentation fetch: {str(e)}")\n        raise HTTPException(status_code=500, detail=f"Documentation fetch failed: {str(e)}")\n\n@app.post("/ask_doc_assistant", tags=["Query"])\nasync def ask_doc_assistant(\n    query_input: QueryInput,\n    doc_manager: DocumentationManager = Depends(get_doc_manager),\n    crew_manager: CrewManager = Depends(get_crew_manager)\n):\n    """\n    Process user queries using RAG with CrewAI-powered analysis.\n    """\n    try:\n        logger.info(f"Processing query: {query_input.query}")\n        query_start_time = datetime.now()\n\n        # Retrieve relevant documentation\n        matching_docs = await doc_manager.retrieve_documents(\n            query=query_input.query,\n            top_k=15\n        )\n\n        if not matching_docs:\n            return {\n                "status": "no_results",\n                "response": "No relevant documentation found for your query.",\n                "query_context": {\n                    "original_query": query_input.query,\n                    "user_context": query_input.user_context\n                },\n                "matching_docs": []\n            }\n\n        # Create and execute CrewAI tasks\n        query_tasks = crew_manager.create_query_tasks(\n            query_input.query,\n            query_input.user_context,\n            matching_docs\n        )\n\n        crew = Crew(\n            agents=[\n                crew_manager.agents[\'retriever\'],\n                crew_manager.agents[\'analyzer\'],\n                crew_manager.agents[\'assistant\']\n            ],\n            tasks=query_tasks,\n            verbose=True\n        )\n        \n        result = crew.kickoff()\n\n        structured_response = {\n            "status": "success",\n            "response": result,\n            "query_context": {\n                "original_query": query_input.query,\n                "user_context": query_input.user_context,\n                "processing_time": (datetime.now() - query_start_time).total_seconds()\n            },\n            "matching_docs": [\n                {\n                    "doc_name": doc["doc_name"],\n                    "similarity_score": doc["similarity_score"],\n                    "processed_at": doc["processed_at"],\n                    "content": doc["content"]  # Return full content\n                }\n                for doc in matching_docs\n            ]\n        }\n\n        logger.info("Query processed successfully")\n        return structured_response\n\n    except Exception as e:\n        logger.error(f"Error processing query: {str(e)}")\n        raise HTTPException(\n            status_code=500,\n            detail={\n                "error": "Query processing failed",\n                "message": str(e),\n                "query": query_input.query\n            }\n        )\n\n@app.delete("/clear_index", tags=["Maintenance"])\nasync def clear_index(\n    doc_manager: DocumentationManager = Depends(get_doc_manager)\n):\n    try:\n        logger.info("Clearing Pinecone index...")\n        doc_manager.index.delete(delete_all=True)\n        doc_manager.pinecone_ids = []\n        with open(VECTOR_IDS_FILE, "w", encoding="utf-8") as f:\n            json.dump([], f)\n        return {"status": "success", "message": "Index cleared successfully"}\n    except Exception as e:\n        logger.error(f"Error clearing index: {str(e)}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get("/sentry-debug", tags=["Sentry"])\nasync def trigger_error():\n    division_by_zero = 1 / 0\n\n\nSENTRY_AUTH_TOKEN = os.getenv("SENTRY_AUTH_TOKEN")\nORG_SLUG = os.getenv("ORG_SLUG")\nPROJECT_SLUG = os.getenv("PROJECT_SLUG")\nGITHUB_WEBHOOK_URL = os.getenv("GITHUB_WEBHOOK_URL")\nGITHUB_PAT = os.getenv("GITHUB_PAT")\n\n# Processed issues (to avoid duplicate branches)\nprocessed_issues = set()\n\nclass SentryAPI:\n    def __init__(self, auth_token: str, organization_slug: str, project_slug: str):\n        self.auth_token = auth_token\n        self.base_url = "https://sentry.io/api/0"\n        self.org_slug = organization_slug\n        self.project_slug = project_slug\n        self.headers = {\n            \'Authorization\': f\'Bearer {auth_token}\',\n            \'Content-Type\': \'application/json\'\n        }\n\n    def _get(self, endpoint: str, params: Optional[Dict] = None) -> Dict:\n        """Helper method for making GET requests to Sentry API."""\n        url = f"{self.base_url}{endpoint}"\n        response = requests.get(url, headers=self.headers, params=params)\n        if response.status_code != 200:\n            raise HTTPException(status_code=response.status_code, detail="Failed to fetch data from Sentry")\n        return response.json()\n\n    def get_latest_issues(self, limit: int = 100) -> List[Dict]:\n        """Fetch the latest unresolved issues from Sentry."""\n        endpoint = f"/projects/{self.org_slug}/{self.project_slug}/issues/"\n        params = {\'limit\': limit, \'query\': \'is:unresolved\'}\n        return self._get(endpoint, params)\n\n    def get_error_location(self, event_id: str) -> Dict:\n        """Get the specific error location including file, line number, and code context."""\n        endpoint = f"/projects/{self.org_slug}/{self.project_slug}/events/{event_id}/"\n        event_data = self._get(endpoint)\n\n        error_location = {\n            \'file\': None,\n            \'line_number\': None,\n            \'context_lines\': [],\n            \'error_line\': None,\n            \'function\': None,\n            \'pre_context\': [],\n            \'post_context\': []\n        }\n\n        if \'entries\' in event_data:\n            for entry in event_data.get(\'entries\', []):\n                if entry.get(\'type\') == \'exception\':\n                    frames = entry.get(\'data\', {}).get(\'values\', [])[0].get(\'stacktrace\', {}).get(\'frames\', [])\n                    if frames:\n                        last_frame = frames[-1]\n                        error_location.update({\n                            \'file\': last_frame.get(\'filename\'),\n                            \'line_number\': last_frame.get(\'lineno\'),\n                            \'context_lines\': last_frame.get(\'context_line\'),\n                            \'error_line\': last_frame.get(\'context_line\'),\n                            \'function\': last_frame.get(\'function\'),\n                            \'pre_context\': last_frame.get(\'pre_context\', []),\n                            \'post_context\': last_frame.get(\'post_context\', [])\n                        })\n        return error_location\n\n    def get_full_error_details(self, issue_id: str) -> Dict:\n        """Get comprehensive error details including stack trace and error location."""\n        endpoint = f"/issues/{issue_id}/events/latest/"\n        event_data = self._get(endpoint)\n\n        error_details = {\n            \'error_type\': None,\n            \'error_message\': None,\n            \'error_location\': None,\n            \'stack_trace\': [],\n            \'timestamp\': None\n        }\n\n        if \'entries\' in event_data:\n            for entry in event_data.get(\'entries\', []):\n                if entry.get(\'type\') == \'exception\':\n                    exception = entry.get(\'data\', {}).get(\'values\', [])[0]\n                    error_details.update({\n                        \'error_type\': exception.get(\'type\'),\n                        \'error_message\': exception.get(\'value\'),\n                        \'timestamp\': event_data.get(\'dateCreated\')\n                    })\n                    frames = exception.get(\'stacktrace\', {}).get(\'frames\', [])\n                    error_details[\'stack_trace\'] = [\n                        {\n                            \'filename\': frame.get(\'filename\'),\n                            \'function\': frame.get(\'function\'),\n                            \'line_number\': frame.get(\'lineno\'),\n                            \'context_line\': frame.get(\'context_line\')\n                        }\n                        for frame in frames\n                    ]\n        error_details[\'error_location\'] = self.get_error_location(event_data.get(\'eventID\'))\n        return error_details\n\n\nclass ErrorDetailResponse(BaseModel):\n    error_type: Optional[str]\n    error_message: Optional[str]\n    error_location: Optional[Dict]\n    stack_trace: List[Dict]\n    timestamp: Optional[str]\n\n\n# Initialize Sentry API\nsentry_api = SentryAPI(auth_token=SENTRY_AUTH_TOKEN, organization_slug=ORG_SLUG, project_slug=PROJECT_SLUG)\n\n\nasync def check_sentry_issues():\n    """Periodically checks for new Sentry issues and creates GitHub branches."""\n    while True:\n        try:\n            issues = sentry_api.get_latest_issues(limit=100)\n            async with httpx.AsyncClient() as client:\n                for issue in issues:\n                    issue_id = issue["id"]\n\n                    if issue_id in processed_issues:\n                        continue  # Skip if the issue was already processed\n\n                    title = issue["title"]\n                    permalink = issue["permalink"]\n                    error_details = sentry_api.get_full_error_details(issue_id)\n\n                    # Prepare payload for GitHub repository dispatch event\n                    webhook_payload = {\n                        "event_type": "create-branch",\n                        "client_payload": {\n                            "branch_name": issue_id,  # Branch name will be the Sentry issue ID\n                            "issue_title": title,\n                            "permalink": permalink\n                        }\n                    }\n\n                    # Send the POST request to trigger the GitHub Action\n                    headers = {\n                        "Authorization": f"token {GITHUB_PAT}",\n                        "Accept": "application/vnd.github.v3+json"\n                    }\n                    webhook_response = await client.post(GITHUB_WEBHOOK_URL, json=webhook_payload, headers=headers)\n\n                    if webhook_response.status_code == 204:\n                        print(f"✅ Branch creation triggered for issue {issue_id}")\n                        processed_issues.add(issue_id)  # Mark issue as processed\n                    else:\n                        print(f"❌ Webhook call failed for issue {issue_id}: {webhook_response.status_code} {webhook_response.text}")\n\n        except Exception as e:\n            print(f"⚠️ Error fetching Sentry issues: {str(e)}")\n\n        await asyncio.sleep(60)  # Wait 60 seconds before checking again\n\n\n@app.on_event("startup")\nasync def startup_event():\n    """Start checking for Sentry issues when FastAPI starts."""\n    asyncio.create_task(check_sentry_issues())\n\n\n@app.get("/status")\nasync def get_status():\n    """Check how many issues have been processed."""\n    return {"processed_issues": list(processed_issues)}\n\n\n@app.get("/get-sentry-issues")\nasync def get_sentry_issues(limit: int = 100):\n    issues = sentry_api.get_latest_issues(limit=limit)\n    formatted_issues = []\n    \n    # Use an asynchronous HTTP client to send webhook calls\n    async with httpx.AsyncClient() as client:\n        for issue in issues:\n            issue_id = issue["id"]\n            title = issue["title"]\n            permalink = issue["permalink"]\n            error_details = sentry_api.get_full_error_details(issue_id)\n\n            formatted_issue = {\n                "id": issue_id,\n                "title": title,\n                "permalink": permalink,\n                "error_type": error_details[\'error_type\'],\n                "error_message": error_details[\'error_message\'],\n                "error_location": error_details[\'error_location\'],\n                "timestamp": error_details[\'timestamp\']\n            }\n            formatted_issues.append(formatted_issue)\n\n            # Prepare payload for GitHub repository dispatch event\n            webhook_payload = {\n                "event_type": "create-branch",\n                "client_payload": {\n                    "branch_name": issue_id,  # The branch name will be the Sentry issue ID\n                    "issue_title": title,\n                    "permalink": permalink\n                }\n            }\n            # Send the POST request to trigger the GitHub Action\n            headers = {\n                "Authorization": f"token {GITHUB_PAT}",\n                "Accept": "application/vnd.github.v3+json"\n            }\n            webhook_response = await client.post(GITHUB_WEBHOOK_URL, json=webhook_payload, headers=headers)\n            if webhook_response.status_code != 204:\n                # GitHub API returns 204 on success for repository_dispatch events\n                print(f"Webhook call failed for issue {issue_id}: {webhook_response.status_code} {webhook_response.text}")\n\n    return {"status": "success", "issues": formatted_issues}\n\n\n@app.get("/get-sentry-error-details/{issue_id}")\nasync def get_error_details(issue_id: str):\n    """Fetches detailed error information for a specific Sentry issue in the same format as `/get-sentry-issues`."""\n    \n    # Fetch error details from Sentry API\n    error_details = sentry_api.get_full_error_details(issue_id)\n    \n    # If issue does not exist, return 404\n    if not error_details.get("error_type"):\n        return {"status": "error", "message": f"Issue {issue_id} not found in Sentry"}\n    \n    # Format the response similar to `/get-sentry-issues`\n    formatted_issue = {\n        "id": issue_id,\n        "title": error_details.get("error_message", "No title available"),\n        "permalink": f"https://sentry.io/organizations/{ORG_SLUG}/issues/{issue_id}/",\n        "error_type": error_details["error_type"],\n        "error_message": error_details["error_message"],\n        "error_location": error_details["error_location"],\n        "timestamp": error_details["timestamp"]\n    }\n\n    return {"status": "success", "issue": formatted_issue}\n\n\nError details: ZeroDivisionError: division by zero - division by zero.\nSteps:\n1. Identify the exact root cause in the given file.\n2. Implement a complete fix that addresses the issue.\n3. Return the ENTIRE fixed file with your changes integrated - not just a snippet.\n4. The fix should be robust, tested, and should prevent similar errors.\n5. Make sure to maintain all imports and functionality from the original code.\n6. Output your final code inside a Python code block. (e.g.